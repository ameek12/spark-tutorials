{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b09d7d1-5daa-4c57-bf77-235c642821be",
   "metadata": {},
   "source": [
    "# __Learning Spark__ (A quick beginners guide)\n",
    "Big data analysis with Spark has been one of the most fun topics I've learned in my Master's program and I want to share all of the goodness! \n",
    "Apache Spark, being a FAST cluster computing platform is the goto for processing and analyzing BIG datasets. This is because single machines don't have enough power to perform computations on large amounts of data. A cluster is a group of computers that combines the resources of many computers together so that we can use all of the combined resources as if it was acting as a single computer. Spark coordinates the work across these computers.  \n",
    "\n",
    "The resource that will teach you the most is: https://www.oreilly.com/library/view/spark-the-definitive/9781491912201/\n",
    "## Spark Concepts\n",
    "A Spark Cluster is where our Spark job executes. Spark applications contain a driver node that contains your program, this node which launches parallel operations on a cluster and the worker nodes, also called executors, execute all of the analysis in parallel. The driver program is basically in the driver seat and manages the workers. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c39a536-470b-41fe-9871-7009e93b73e5",
   "metadata": {},
   "source": [
    "# Let's code! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7bb2c19a-4d20-4b7d-a891-17330c5c0bbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/07/29 06:34:11 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# The SparkContext object allows the driver programs access Spark and acts as a connection to a computing cluster. \n",
    "\n",
    "# Import Dependencies\n",
    "import os\n",
    "import sys\n",
    "os.environ[\"PYSPARK_PYTHON\"] = sys.executable\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = sys.executable\n",
    "\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Using SparkContext vs SparkSession\n",
    "# Creating a SparkContext instance (driver program) to control your Spark application. \n",
    "# Note: This is may be depricated in future instances \n",
    "sc = SparkContext()\n",
    "\n",
    "# You can also create a SparkSession instance (driver program) to control your Spark application. \n",
    "spark = SparkSession.builder.appName('Practice').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b16e3af-6d27-4b2f-9f9d-a5ae0289596e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Resilient Distributed Datasets (RDD) - What are they? \n",
    "* RDDs is an immutable distributed collection of elements which is split into partitions that may be computed on different nodes. \n",
    "* RDD Operations have Tranformations which result in another RDD or Actions which compute a single result.\n",
    "* Almost all of the code you run in PySpark DataFrames will be reduced to an RDD; however DataFrames are more efficient and stable than creating manual RDDs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9326bbe-c40c-4d5e-93ae-e8504483d3af",
   "metadata": {},
   "source": [
    "## Transformations return a new RDD\n",
    "## Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9130c9cb-eeb7-4b7a-acfa-12d74c8d506d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets create an rdd! \n",
    "import numpy as np\n",
    "text_array = np.array([[\"The Cat In The Hat\"],[\"Hop On Pop\"],[\"Are You My Mother?\"],[\"Green Eggs And Ham\"]])\n",
    "rdd = sc.parallelize(text_array).persist() # Persist helps it stay in memory and makes everything faster. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a7a5f5c-38eb-476a-868f-f2334f4bf4d6",
   "metadata": {},
   "source": [
    "# map()\n",
    "### Applies a function to each element of the current RDD and returns a new RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a1599a15-8df3-440f-9cbd-d2367244e203",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['The', 'Cat', 'In', 'The', 'Hat'],\n",
       " ['Hop', 'On', 'Pop'],\n",
       " ['Are', 'You', 'My', 'Mother?'],\n",
       " ['Green', 'Eggs', 'And', 'Ham']]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.map(lambda title: title[0].split(\" \")).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0238cf27-9637-4eab-b6ba-01c1cdcf15d9",
   "metadata": {},
   "source": [
    "# flatMap()\n",
    "## Similar to map, but each input item can be mapped to 0 or more output items (so func should return a Seq rather than a single item)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a205eb7-1829-4d0f-89f0-f78f6b22beb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'Cat',\n",
       " 'In',\n",
       " 'The',\n",
       " 'Hat',\n",
       " 'Hop',\n",
       " 'On',\n",
       " 'Pop',\n",
       " 'Are',\n",
       " 'You',\n",
       " 'My',\n",
       " 'Mother?',\n",
       " 'Green',\n",
       " 'Eggs',\n",
       " 'And',\n",
       " 'Ham']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Flatten all lists into a single list of words\n",
    "rdd.flatMap(lambda title: title[0].split(\" \")).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c769365-09be-41cd-975d-1948b9fc62c3",
   "metadata": {},
   "source": [
    "# filter()\n",
    "## If you are familiar with SQL this is almost like applying a WHERE clause. filter() returns a new RDD by selecting those elements of the source RDD on where the function returns true."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "843f0c9f-15e6-4a86-b7d2-d23b3c9de708",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['My', 'Mother?']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can chain (lazy evaluate) the transformations together. \n",
    "# Here, we are looking for the words from the title that start with 'M' \n",
    "rdd.flatMap(lambda title: title[0].split(\" \")).filter(lambda word: word[0]==\"M\").collect()\n",
    "## Uncomment below to see all of the words which have 'a' as the second letter in the word. \n",
    "# rdd.flatMap(lambda title: title[0].split(\" \")).filter(lambda word: word[1]==\"a\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9cea8900-d359-4529-a7da-ac17cfc8020f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['My', 'Mother?']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Could also use 'startswith' \n",
    "rdd.flatMap(lambda x: x[0].split(\" \")).filter(lambda word: word.startswith(\"M\")).collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab251d4e-1637-448d-a33e-65ad3c266774",
   "metadata": {},
   "source": [
    "# Joins! Joining RDDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "24328df2-7c70-4661-989f-b6f6bfbae6a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Id,First_Name,Last_Name',\n",
       " '1,Homer,Hill',\n",
       " '2,Margie,Hill',\n",
       " '3,Bartholomew ,Hill',\n",
       " '4,Hank,Simpson',\n",
       " '5,Peggy,Simpson',\n",
       " '6,Bobby ,Simpson']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets download a file!\n",
    "rdd = sc.textFile(\"./people.csv\")\n",
    "rdd2 = sc.textFile(\"./cust_foods.csv\")\n",
    "rdd.collect()\n",
    "#print(rdd2.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c14a2cb5-fd25-4fae-9adb-113e84ea363a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume the first line is the header\n",
    "#header = rdd.first()\n",
    "\n",
    "# Filter out the header to avoid processing it as data\n",
    "#data = rdd.filter(lambda line: line != header)\n",
    "\n",
    "rdd_split=rdd.map(lambda line: line.split(\",\"))\n",
    "rdd2_split=rdd2.map(lambda line: line.split(\",\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4865b84f-dda8-4829-9082-b5e5f159c8c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Id', 'First_Name', 'Last_Name'],\n",
       " ['1', 'Homer', 'Hill'],\n",
       " ['2', 'Margie', 'Hill'],\n",
       " ['3', 'Bartholomew ', 'Hill'],\n",
       " ['4', 'Hank', 'Simpson'],\n",
       " ['5', 'Peggy', 'Simpson'],\n",
       " ['6', 'Bobby ', 'Simpson']]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_split.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a4f89cf6-1fb4-4f33-aec0-bf92167ac176",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['cust_id', 'Item', 'Cost', 'Qty'],\n",
       " ['1', 'Donuts', '7.99', '2'],\n",
       " ['2', 'Bourbon', '15.49', '1'],\n",
       " ['3', 'Slushies', '4.99', '1'],\n",
       " ['4', 'Steak', '18.95', '3'],\n",
       " ['5', 'Turkey', '15.67', '1'],\n",
       " ['6', 'Deli Meats', '5.99', '5']]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd2_split.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d9c7aac-75c6-45e4-848b-47e08deaca1d",
   "metadata": {},
   "source": [
    "## Before we join, we need to transform the RDD into (Key, Value) pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "369f3f69-1725-4771-ad4c-c30d633e0f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforming to key value pairs\n",
    "rdd_pairs = rdd_split.map(lambda x: (x[0], (x[1], x[2])))\n",
    "rdd2_pairs = rdd2_split.map(lambda x: (x[0], (x[1], x[2], x[3])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8c3bfe0f-b88c-4d83-ae6e-f5c9389a851a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('4', (('Hank', 'Simpson'), ('Steak', '18.95', '3'))),\n",
       " ('3', (('Bartholomew ', 'Hill'), ('Slushies', '4.99', '1'))),\n",
       " ('6', (('Bobby ', 'Simpson'), ('Deli Meats', '5.99', '5'))),\n",
       " ('1', (('Homer', 'Hill'), ('Donuts', '7.99', '2'))),\n",
       " ('2', (('Margie', 'Hill'), ('Bourbon', '15.49', '1'))),\n",
       " ('5', (('Peggy', 'Simpson'), ('Turkey', '15.67', '1')))]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Perform a join\n",
    "joined_rdd=rdd_pairs.join(rdd2_pairs)\n",
    "joined_rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed18d277-fb9d-4013-a762-67b9c24ddca1",
   "metadata": {},
   "source": [
    "## READY SET ACTION! \n",
    "### Actions reduce to a single value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a941f08-4e5f-441c-85a2-3214281b404b",
   "metadata": {},
   "source": [
    "## Reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2c70b36d-ac53-47a8-a59f-c236864361f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sum of squares example! \n",
    "# Create a RDD of numbers\n",
    "rdd_ss = sc.parallelize([1,2,3,4])\n",
    "# Square each using map \n",
    "squared_rdd = rdd_ss.map(lambda x: x*x)\n",
    "# reduce or add up all of the numbers squared \n",
    "reduced = squared_rdd.reduce(lambda x,y: x+y)\n",
    "reduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "020bcb12-0886-4084-b6dc-8cf52b529cd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('now', 2),\n",
       " ('always', 1),\n",
       " ('got', 1),\n",
       " ('own', 1),\n",
       " ('never', 1),\n",
       " ('really', 1),\n",
       " ('cared', 1),\n",
       " ('until', 1),\n",
       " ('And', 1),\n",
       " ('do', 2),\n",
       " ('Till', 1),\n",
       " ('I', 5),\n",
       " ('by', 1),\n",
       " ('on', 1),\n",
       " ('my', 1),\n",
       " ('met', 1),\n",
       " ('you', 3),\n",
       " ('it', 1),\n",
       " ('chills', 1),\n",
       " ('me', 1),\n",
       " ('to', 1),\n",
       " ('the', 1),\n",
       " ('bone', 1),\n",
       " ('How', 2),\n",
       " ('get', 2),\n",
       " ('alone', 2)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reduce by key - does exactly what it sounds like\n",
    "heart_rdd = sc.textFile('./heart.txt')\n",
    "heart_flat=heart_rdd.flatMap(lambda x: x.split(\" \"))\n",
    "heart = heart_flat.map(lambda x:(x,1))\n",
    "heart.reduceByKey(lambda x,y:x+y).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "09b5c4f2-6f32-44fa-a8ba-aa71eb9e0e10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {'Till': 1,\n",
       "             'now': 2,\n",
       "             'I': 5,\n",
       "             'always': 1,\n",
       "             'got': 1,\n",
       "             'by': 1,\n",
       "             'on': 1,\n",
       "             'my': 1,\n",
       "             'own': 1,\n",
       "             'never': 1,\n",
       "             'really': 1,\n",
       "             'cared': 1,\n",
       "             'until': 1,\n",
       "             'met': 1,\n",
       "             'you': 3,\n",
       "             'And': 1,\n",
       "             'it': 1,\n",
       "             'chills': 1,\n",
       "             'me': 1,\n",
       "             'to': 1,\n",
       "             'the': 1,\n",
       "             'bone': 1,\n",
       "             'How': 2,\n",
       "             'do': 2,\n",
       "             'get': 2,\n",
       "             'alone': 2})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# countByKey()\n",
    "# essentially the same as above\n",
    "heart.countByKey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9e485aa5-1ef9-4558-b51b-6d792ff4ce0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Counting the items in the RDD\n",
    "rdd_ss.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ecdbdf3f-ef2d-44bf-b0a2-c0c0b02f29e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_ss.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "51eed462-3833-4760-a442-4edb385dfa3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_ss.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "185ecb1f-1326-4304-82c8-ddd254a40d2d",
   "metadata": {},
   "source": [
    "## Using DataFrames in Spark!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "64b8234c-6720-4673-b50f-1a3e2705b72a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv('./cust_foods.csv', inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d02db629-e342-4a40-9e1b-46af574a72b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+-----+---+\n",
      "|cust_id|      Item| Cost|Qty|\n",
      "+-------+----------+-----+---+\n",
      "|      1|    Donuts| 7.99|  2|\n",
      "|      2|   Bourbon|15.49|  1|\n",
      "|      3|  Slushies| 4.99|  1|\n",
      "|      4|     Steak|18.95|  3|\n",
      "|      5|    Turkey|15.67|  1|\n",
      "|      6|Deli Meats| 5.99|  5|\n",
      "+-------+----------+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81816918-faf7-4658-a025-16975828820c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Aggregations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1a23a41a-9c5c-40e3-b438-154780fea541",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|     stddev(Cost)|\n",
      "+-----------------+\n",
      "|5.896960799146172|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import countDistinct, avg, stddev \n",
    "df.select(stddev('Cost')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6670389a-1af4-41a4-bd25-4c0019e7af0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|         avg(Cost)|\n",
      "+------------------+\n",
      "|11.513333333333334|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(avg('Cost')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "435a7c03-195e-4234-8725-88356e5dc0a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|count(DISTINCT Cost)|\n",
      "+--------------------+\n",
      "|                   6|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(countDistinct('Cost')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "343ba54c-7f0b-42f0-92ae-490ef27ef4db",
   "metadata": {},
   "source": [
    "## Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8d153ec2-fe6f-4ef2-93c5-799843bd4013",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+\n",
      "|      Item|Cost|\n",
      "+----------+----+\n",
      "|    Donuts|7.99|\n",
      "|  Slushies|4.99|\n",
      "|Deli Meats|5.99|\n",
      "+----------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(df['Cost'] < 8).select(['Item', 'Cost']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9e9ad414-68f8-41a4-aff6-90ce9f7f5522",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "|   Item| Cost|\n",
      "+-------+-----+\n",
      "| Donuts| 7.99|\n",
      "|Bourbon|15.49|\n",
      "| Turkey|15.67|\n",
      "+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Select rows with cost price less than 16 and cost greater than 7 \n",
    "df.filter((df['Cost'] < 16) & (df['Cost'] > 7)).select(['Item', 'Cost']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d900518-45ab-4914-bc18-e0dd56150398",
   "metadata": {},
   "source": [
    "## Adding New Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b8134c69-5d13-45ba-8712-a5010929e855",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+-----+---+------------------+\n",
      "|cust_id|      Item| Cost|Qty|             Total|\n",
      "+-------+----------+-----+---+------------------+\n",
      "|      1|    Donuts| 7.99|  2|             15.98|\n",
      "|      2|   Bourbon|15.49|  1|             15.49|\n",
      "|      3|  Slushies| 4.99|  1|              4.99|\n",
      "|      4|     Steak|18.95|  3|56.849999999999994|\n",
      "|      5|    Turkey|15.67|  1|             15.67|\n",
      "|      6|Deli Meats| 5.99|  5|29.950000000000003|\n",
      "+-------+----------+-----+---+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"Total\", df['Cost'] * df['Qty']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8568ae3-239f-4558-86ab-2a6aa660e282",
   "metadata": {},
   "source": [
    "## sort and orderBy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "40136e26-5fff-4488-9201-87d105f37040",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+-----+---+\n",
      "|cust_id|      Item| Cost|Qty|\n",
      "+-------+----------+-----+---+\n",
      "|      2|   Bourbon|15.49|  1|\n",
      "|      3|  Slushies| 4.99|  1|\n",
      "|      5|    Turkey|15.67|  1|\n",
      "|      1|    Donuts| 7.99|  2|\n",
      "|      4|     Steak|18.95|  3|\n",
      "|      6|Deli Meats| 5.99|  5|\n",
      "+-------+----------+-----+---+\n",
      "\n",
      "+-------+----------+-----+---+\n",
      "|cust_id|      Item| Cost|Qty|\n",
      "+-------+----------+-----+---+\n",
      "|      2|   Bourbon|15.49|  1|\n",
      "|      6|Deli Meats| 5.99|  5|\n",
      "|      1|    Donuts| 7.99|  2|\n",
      "|      3|  Slushies| 4.99|  1|\n",
      "|      4|     Steak|18.95|  3|\n",
      "|      5|    Turkey|15.67|  1|\n",
      "+-------+----------+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.sort(df['Qty']).show()\n",
    "df.orderBy(df['Item']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d870465f-4a65-43a4-ad0f-4c1447ae371c",
   "metadata": {},
   "source": [
    "## Using Spark SQL! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "3ba5ed57-5c4b-4d64-aa3d-1b8c9097267c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register as a sql temp view of a table called people\n",
    "car_df = spark.read.csv('cars.csv', inferSchema=True, header=True)\n",
    "# Drop columns \n",
    "car_df = car_df.drop(\"_c4\")\n",
    "car_df = car_df.drop(\"_c5\")\n",
    "car_df = car_df.drop(\"_c6\")\n",
    "car_df.createOrReplaceTempView('cars')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "1d813a08-74c6-455b-bfc7-c5c3b04d8f90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------+------+------+\n",
      "| Id|         car|person|  cost|\n",
      "+---+------------+------+------+\n",
      "|  1|    Corvette|  Katy|100000|\n",
      "|  2|      Camero|  Ryan| 50000|\n",
      "|  3|Lamborghini | Kevin|160000|\n",
      "|  4|    Ferarri |  Ryan|200000|\n",
      "|  5|          GT|  Katy|  NULL|\n",
      "|  6|     Mustang| Kevin| 50000|\n",
      "+---+------------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Now we can use pure SQL to query through\n",
    "query1 = spark.sql(\"SELECT * FROM cars\")\n",
    "query1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "6dda700b-f1f6-47fd-80fc-7eeb15f525e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+------+-----+\n",
      "| Id|    car|person| cost|\n",
      "+---+-------+------+-----+\n",
      "|  2| Camero|  Ryan|50000|\n",
      "|  6|Mustang| Kevin|50000|\n",
      "+---+-------+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query2 = spark.sql(\"SELECT * FROM cars WHERE cost < 100000\")\n",
    "query2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb031d50-fd42-40cf-9518-ce5e33b777b3",
   "metadata": {},
   "source": [
    "## Group By"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "ae95f2cf-c3d1-40c2-bcf8-8be8310a02b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+\n",
      "|person|avg(cost)|\n",
      "+------+---------+\n",
      "| Kevin| 105000.0|\n",
      "|  Ryan| 125000.0|\n",
      "|  Katy| 100000.0|\n",
      "+------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query3 = spark.sql(\"SELECT person,avg(cost) FROM cars GROUP BY person\")\n",
    "query3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "8dadbd31-b2b0-433b-8137-f7e30263819e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+\n",
      "|person|avg(cost)|\n",
      "+------+---------+\n",
      "| Kevin| 105000.0|\n",
      "|  Ryan| 125000.0|\n",
      "|  Katy| 100000.0|\n",
      "+------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# In dataframes: Groups by the average cost of each person's car\n",
    "car_df.groupBy(\"person\").avg(\"cost\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9219c1a3-c8d7-4a55-b9fe-80396ad26a27",
   "metadata": {},
   "source": [
    "## Joins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "9e5ca6d0-aba2-48e9-9b7e-0d65411208f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inner Join\n",
    "people_df = spark.read.csv('people.csv', inferSchema=True, header=True)\n",
    "food_df = spark.read.csv('cust_foods.csv', inferSchema=True, header=True)\n",
    "people_df.createOrReplaceTempView('people')\n",
    "food_df.createOrReplaceTempView('cust_food')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "f64ecf9e-4ed2-40b5-8cb7-416d861d70f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------+---------+\n",
      "| Id|  First_Name|Last_Name|\n",
      "+---+------------+---------+\n",
      "|  1|       Homer|     Hill|\n",
      "|  2|      Margie|     Hill|\n",
      "|  3|Bartholomew |     Hill|\n",
      "|  4|        Hank|  Simpson|\n",
      "|  5|       Peggy|  Simpson|\n",
      "|  6|      Bobby |  Simpson|\n",
      "+---+------------+---------+\n",
      "\n",
      "+-------+----------+-----+---+\n",
      "|cust_id|      Item| Cost|Qty|\n",
      "+-------+----------+-----+---+\n",
      "|      1|    Donuts| 7.99|  2|\n",
      "|      2|   Bourbon|15.49|  1|\n",
      "|      3|  Slushies| 4.99|  1|\n",
      "|      4|     Steak|18.95|  3|\n",
      "|      5|    Turkey|15.67|  1|\n",
      "|      6|Deli Meats| 5.99|  5|\n",
      "+-------+----------+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "people_df.show()\n",
    "food_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "e8a46391-3a18-4352-984e-29f8fb82b67d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------+---------+-------+----------+-----+---+\n",
      "| Id|  First_Name|Last_Name|cust_id|      Item| Cost|Qty|\n",
      "+---+------------+---------+-------+----------+-----+---+\n",
      "|  1|       Homer|     Hill|      1|    Donuts| 7.99|  2|\n",
      "|  2|      Margie|     Hill|      2|   Bourbon|15.49|  1|\n",
      "|  3|Bartholomew |     Hill|      3|  Slushies| 4.99|  1|\n",
      "|  4|        Hank|  Simpson|      4|     Steak|18.95|  3|\n",
      "|  5|       Peggy|  Simpson|      5|    Turkey|15.67|  1|\n",
      "|  6|      Bobby |  Simpson|      6|Deli Meats| 5.99|  5|\n",
      "+---+------------+---------+-------+----------+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Performing Inner Join\n",
    "table = spark.sql(\"SELECT * FROM people as p INNER JOIN cust_food as f ON p.id = f.cust_id\")\n",
    "table.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "c5a710ee-4a3d-44d3-9253-d89c03f3b1f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------+---------+----------+-----+\n",
      "| Id|  First_Name|Last_Name|      Item| Cost|\n",
      "+---+------------+---------+----------+-----+\n",
      "|  1|       Homer|     Hill|    Donuts| 7.99|\n",
      "|  2|      Margie|     Hill|   Bourbon|15.49|\n",
      "|  3|Bartholomew |     Hill|  Slushies| 4.99|\n",
      "|  4|        Hank|  Simpson|     Steak|18.95|\n",
      "|  5|       Peggy|  Simpson|    Turkey|15.67|\n",
      "|  6|      Bobby |  Simpson|Deli Meats| 5.99|\n",
      "+---+------------+---------+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Left Outer Join\n",
    "table2 = spark.sql(\"\"\"\n",
    "    SELECT p.*, f.Item, f.Cost\n",
    "    FROM people as p\n",
    "    LEFT OUTER JOIN cust_food as f\n",
    "    ON p.id = f.cust_id\n",
    "\"\"\")\n",
    "table2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2162f4a-1dd6-4e93-9c45-89bc753d4eba",
   "metadata": {},
   "source": [
    "## Subqueries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "1ef3ba35-8f5a-4a57-9f9a-23d287413632",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|  first_name|\n",
      "+------------+\n",
      "|       Homer|\n",
      "|      Margie|\n",
      "|Bartholomew |\n",
      "|       Peggy|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Find the the spend less than $20 on a meal. \n",
    "subquery1 = spark.sql(\"\"\"\n",
    "    SELECT p.first_name\n",
    "    FROM people AS p\n",
    "    INNER JOIN (\n",
    "        SELECT cust_id\n",
    "        FROM (\n",
    "            SELECT cust_id, cost * qty AS total_cost\n",
    "            FROM cust_food\n",
    "        ) AS cf\n",
    "        WHERE total_cost < 20\n",
    "    ) AS filtered_cf\n",
    "    ON p.id = filtered_cf.cust_id\n",
    "\"\"\")\n",
    "subquery1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc37fcd9-dedc-4b2b-acb7-149e2d080357",
   "metadata": {},
   "source": [
    "# Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a18cb4b7-be61-4891-8c3d-209d7a2042a5",
   "metadata": {},
   "source": [
    "1. Find the number of items in the list where the length is 3. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a3174bd4-f34f-4e81-b2b3-34b556977b48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ago',\n",
       " 'was',\n",
       " 'run',\n",
       " 'But',\n",
       " 'try',\n",
       " 'not',\n",
       " 'you',\n",
       " 'man',\n",
       " 'You',\n",
       " 'me,',\n",
       " 'get',\n",
       " \"I'm\"]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Answer:\n",
    "rdd = sc.textFile(\"./problem1.txt\")\n",
    "# Take out the quots then split and filter.\n",
    "rdd.map(lambda x: x.replace('\"', '')).flatMap(lambda x: x.split(\" \")).filter(lambda x: len(x)==3).distinct().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f1c192-2b01-43a0-bcd5-0975450a9791",
   "metadata": {},
   "source": [
    "2. List the first 2 letters of a words in the RDD and count them and sort them in ascending order. (continue using problem1.txt) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "71e872e1-42f5-47e1-8d85-dc5292419186",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {'A': 1,\n",
       "             'Bu': 1,\n",
       "             'Co': 2,\n",
       "             'He': 1,\n",
       "             'I': 3,\n",
       "             \"I'\": 1,\n",
       "             'Se': 1,\n",
       "             'Tr': 1,\n",
       "             'We': 1,\n",
       "             'Wh': 1,\n",
       "             'Yo': 2,\n",
       "             'a': 3,\n",
       "             'ag': 1,\n",
       "             'aw': 2,\n",
       "             'bl': 1,\n",
       "             'ca': 1,\n",
       "             'co': 1,\n",
       "             'do': 1,\n",
       "             'dr': 1,\n",
       "             'ea': 1,\n",
       "             'ey': 1,\n",
       "             'ge': 1,\n",
       "             'gi': 1,\n",
       "             'ha': 1,\n",
       "             'he': 3,\n",
       "             'hi': 1,\n",
       "             'ho': 1,\n",
       "             'in': 1,\n",
       "             'it': 1,\n",
       "             'kn': 3,\n",
       "             'la': 1,\n",
       "             'le': 1,\n",
       "             'li': 1,\n",
       "             'lo': 3,\n",
       "             'ma': 3,\n",
       "             'me': 4,\n",
       "             'ne': 1,\n",
       "             'ni': 1,\n",
       "             'no': 2,\n",
       "             'on': 1,\n",
       "             'ot': 1,\n",
       "             'pr': 1,\n",
       "             'ri': 1,\n",
       "             'ru': 1,\n",
       "             'sa': 1,\n",
       "             'se': 3,\n",
       "             'sm': 1,\n",
       "             'so': 3,\n",
       "             'st': 1,\n",
       "             'th': 1,\n",
       "             'to': 5,\n",
       "             'tr': 4,\n",
       "             'un': 3,\n",
       "             'wa': 1,\n",
       "             'wi': 1,\n",
       "             'ye': 2,\n",
       "             'yo': 1})"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Answer\n",
    "rdd_split = rdd.map(lambda x: x.replace('\"', '')).flatMap(lambda x: x.split(\" \")).map(lambda word: (word[:2],1)).sortByKey()\n",
    "rdd_split.countByKey()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e55f704b-42a1-436a-861f-a94d7369de26",
   "metadata": {},
   "source": [
    "3. Fill in missing null values of the dataframe items with the average value. (Use cars.csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "33b40bf4-62c4-4fe6-877e-5ccdf1dcac4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------+------+------+----+----+----+\n",
      "| Id|         car|person|  cost| _c4| _c5| _c6|\n",
      "+---+------------+------+------+----+----+----+\n",
      "|  1|    Corvette|  Katy|100000|NULL|NULL|NULL|\n",
      "|  2|      Camero|  Ryan| 50000|NULL|NULL|NULL|\n",
      "|  3|Lamborghini | Kevin|160000|NULL|NULL|NULL|\n",
      "|  4|    Ferarri |  Ryan|200000|NULL|NULL|NULL|\n",
      "|  5|          GT|  Katy|  NULL|NULL|NULL|NULL|\n",
      "|  6|     Mustang| Kevin| 50000|NULL|NULL|NULL|\n",
      "+---+------------+------+------+----+----+----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/07/29 08:25:56 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: Id, car, person, cost, , , \n",
      " Schema: Id, car, person, cost, _c4, _c5, _c6\n",
      "Expected: _c4 but found: \n",
      "CSV file: file:///Users/ameek/Documents/UCSD_Q3/cars.csv\n"
     ]
    }
   ],
   "source": [
    "car_df = spark.read.csv('cars.csv', inferSchema=True, header=True)\n",
    "car_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "63ec11ca-4dfa-40c8-9ac9-44e8b011be60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns \n",
    "car_df = car_df.drop(\"_c4\")\n",
    "car_df = car_df.drop(\"_c5\")\n",
    "car_df = car_df.drop(\"_c6\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "7a3b434f-c8be-4232-9356-151843753c5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------+------+------+\n",
      "| Id|         car|person|  cost|\n",
      "+---+------------+------+------+\n",
      "|  1|    Corvette|  Katy|100000|\n",
      "|  2|      Camero|  Ryan| 50000|\n",
      "|  3|Lamborghini | Kevin|160000|\n",
      "|  4|    Ferarri |  Ryan|200000|\n",
      "|  5|          GT|  Katy|  NULL|\n",
      "|  6|     Mustang| Kevin| 50000|\n",
      "+---+------------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "car_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "34bb9ba0-6258-46e8-afd3-7de7c0ca49b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|avg(cost)|\n",
      "+---------+\n",
      "| 112000.0|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "avg_cost= car_df.select(avg('cost'))#.collect()\n",
    "avg_cost.show()\n",
    "avg_cost= avg_cost.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "74365709-cbe0-4e86-a51d-78d24aab7972",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "112000.0"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_cost_val= avg_cost[0][0]\n",
    "avg_cost_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "7df30dd9-0a65-4396-a307-8f01fa732b98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------+------+------+\n",
      "| Id|         car|person|  cost|\n",
      "+---+------------+------+------+\n",
      "|  1|    Corvette|  Katy|100000|\n",
      "|  2|      Camero|  Ryan| 50000|\n",
      "|  3|Lamborghini | Kevin|160000|\n",
      "|  4|    Ferarri |  Ryan|200000|\n",
      "|  5|          GT|  Katy|112000|\n",
      "|  6|     Mustang| Kevin| 50000|\n",
      "+---+------------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Fill in the NULL with the average!\n",
    "car_df.na.fill(avg_cost_val, subset=['cost']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d0b068-3985-490b-9ad1-a1b94bddd4c0",
   "metadata": {},
   "source": [
    "# Part 2 will be Machine Learning with Spark! "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
